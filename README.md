# EconLab-Patent-Network-Centrality
Python code to process the US patent citation network over time to produce centrality measures and network visualizations of patent categories.

## Dependencies
* Python 2.7
* Msgpack-numpy
* NetworkX
* Matplotlib
* Pandas
* Numpy

## Folders

### data
Contains all the original CSV patent citation data.

### cache
Stores the msgpack serialization files used to save and load the processed data. Serialization post-processed data is important because the process step can take up to a day to finish running (due to the size of the data).

### outputs
Stores all the outputs generated by network_analysis.py. This includes .png files of the network graphs, heatmaps, and centrality rankings over time. this also includes CSV files that contain centrality rankings for each patent category.

## Files

### network_creator.py
This file loads in all the original patent citation data and processes them to create adjacency matrices and vectors. This file also creates a crosswalk dictionary that links uspto values to ipc values.

At the top of the file are some parameters that can be changed, such as the starting year and the ending year for generating these network matrices and vectors. The year_gap parameter is used to specify how many years of future citing patents should be considered as linked to a patent. 

Note that this file should only be called once (every time parameters are changed), as the outputs are serialized as msgpack files and saved in the cache folder. Generating these adjacency matrices and vectors can take a long time due to the sheer size of the input data.

### network_analysis.py
